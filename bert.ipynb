{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c54d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 16:24:44.578886: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-22 16:24:44.743995: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-07-22 16:24:44.751253: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-22 16:24:44.751273: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-07-22 16:24:44.796646: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-07-22 16:24:45.577585: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-07-22 16:24:45.577685: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-07-22 16:24:45.577696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import cuda\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3580cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46b55a",
   "metadata": {},
   "source": [
    "## Общий препроцессинг корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e4774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df = df.rename(columns={'Текст Сообщения':'text', 'Категория': 'label'})\n",
    "test_dataset = pd.read_csv(\"test.csv\")\n",
    "test_dataset = test_dataset.rename(columns={'Текст Сообщения':'text'})\n",
    "df = pd.concat([df, test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "335c54f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    '''Очистка текста от html'''\n",
    "    cleantext = re.sub(CLEANR, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "df['text'] = df['text'].apply(cleanhtml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9ea50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LabelEncoder для столбца с тематикой\n",
    "le_theme = preprocessing.LabelEncoder()\n",
    "le_theme.fit(df['Тематика'])\n",
    "# df['Тематика'] = le_theme.transform(df['Тематика'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb8cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')\n",
    "\n",
    "def strip_newline(series):\n",
    "    '''Разделитель строк'''\n",
    "    return [review.replace('\\n','') for review in series]\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    '''Токенайзер'''\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    '''Очистка от стопслов'''\n",
    "    out = [[word for word in simple_preprocess(str(doc))\n",
    "            if word not in stop_words]\n",
    "            for doc in texts]\n",
    "    return out\n",
    "\n",
    "def bigrams(words, bi_min=5):\n",
    "    '''Выбор слов на основании биграм'''\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod\n",
    "\n",
    "def get_corpus(df):\n",
    "    '''Подготовка корпуса'''\n",
    "    df['text'] = strip_newline(df.text)\n",
    "    words = list(sent_to_words(df.text))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram_mod = bigrams(words)\n",
    "    bigram = [bigram_mod[review] for review in words]    \n",
    "    return bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb773425",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_train = get_corpus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5c6d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Преобразование текстов в разреженную матрицу весов TF/IDF\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False)    \n",
    "tfidf = tfidf.fit(bigram_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6192c86",
   "metadata": {},
   "source": [
    "## Препроцессинг train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbd7b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df = df.rename(columns={'Текст Сообщения':'text', 'Категория': 'label'})\n",
    "df['text'] = df['text'].apply(cleanhtml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8434ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upsampling обучающей выборки\n",
    "MAX_TEXT_LENGTH = 256 #Максимальная длина текста \n",
    "head_part = df[\"text\"].str.slice(0, MAX_TEXT_LENGTH)\n",
    "tail_part = df[df[\"text\"].str.len() > MAX_TEXT_LENGTH][\"text\"].str.slice(MAX_TEXT_LENGTH)\n",
    "\n",
    "while len(tail_part):\n",
    "    head_part = pd.concat([head_part, tail_part.str.slice(0, MAX_TEXT_LENGTH)])\n",
    "    tail_part = tail_part[tail_part.str.len() > MAX_TEXT_LENGTH].str.slice(MAX_TEXT_LENGTH)\n",
    "\n",
    "df = df[[\"label\", \"Ответственное лицо\", \"Тематика\"]].merge(pd.concat([head_part, tail_part]), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459bb41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Тематика'] = le_theme.transform(df['Тематика']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c84be520",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_train = get_corpus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7c0131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf.transform(bigram_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "045efde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3848, 22175)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62986dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid(sparse, serie):\n",
    "    '''Определение ближайшего центроида кластера тематики к тексту'''\n",
    "    res=[]\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(sparse, serie)\n",
    "    for row in sparse:\n",
    "        res.append(clf.predict(row)[0])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4aabe426",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['theme'] = le_theme.inverse_transform(get_centroid(X_train_tfidf, df['Тематика']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4255844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9397089397089398"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df['Тематика'], le_theme.transform(df['theme']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ea2473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Объединяем текст с колонкой theme, а не с колонкой тематики, т.к. она теперь носит условный характер\n",
    "df['text'] = df['theme']+' '+df[\"text\"]\n",
    "df = df[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b295d5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Нарушения, связанные с содержанием электросети...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Аварийные деревья По фасаду дома по адресу ул....</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Аварийные деревья жет просто сломаться. А это ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Аварийные деревья ахнуть газом, будет уже позд...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Безнадзорные животные Агресивные собаки. На ра...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Парковки на дорогах в границах городских округ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Парковки на дорогах в границах городских округ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Аварийные деревья Состоят 3 засохшие дерева (2...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Нарушение дорожного покрытия (ямы)  на дорогах...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>ПЦР-тест, тест на антитела Сдан ПЦР-тест 29.10...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3848 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     Нарушения, связанные с содержанием электросети...      3\n",
       "1     Аварийные деревья По фасаду дома по адресу ул....      3\n",
       "1     Аварийные деревья жет просто сломаться. А это ...      3\n",
       "1     Аварийные деревья ахнуть газом, будет уже позд...      3\n",
       "2     Безнадзорные животные Агресивные собаки. На ра...      1\n",
       "...                                                 ...    ...\n",
       "1996  Парковки на дорогах в границах городских округ...      0\n",
       "1996  Парковки на дорогах в границах городских округ...      0\n",
       "1997  Аварийные деревья Состоят 3 засохшие дерева (2...      3\n",
       "1998  Нарушение дорожного покрытия (ямы)  на дорогах...      0\n",
       "1999  ПЦР-тест, тест на антитела Сдан ПЦР-тест 29.10...      4\n",
       "\n",
       "[3848 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc79258a",
   "metadata": {},
   "source": [
    "## Препроцессинг test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c58cc277",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test.csv\")\n",
    "test_data = test_data.rename(columns={'Текст Сообщения':'text'})\n",
    "test_data['text'] = test_data['text'].apply(cleanhtml)\n",
    "bigram_train = get_corpus(test_dataset)\n",
    "X_test_tfidf = tfidf.transform(bigram_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fca3617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Тематика'] = le_theme.transform(test_data['Тематика'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3c93ba4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_data['theme'] = le_theme.inverse_transform(get_centroid(X_test_tfidf, test_data['Тематика']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3446145d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.991"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Точность с которой мы определяем тематику\n",
    "accuracy_score(test_dataset['Тематика'], test_data['theme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "397dc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выбираем только колонку с текстом, чтобы в тест не попали лики\n",
    "test_data['text'] = test_data['theme']+' '+test_data[\"text\"] \n",
    "test_data = test_data[['text']]\n",
    "test_data['ENCODE_CAT'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4caa4c",
   "metadata": {},
   "source": [
    "## BERT Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08eb9e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_dict = {}\n",
    "\n",
    "def encode_cat(x):\n",
    "    \"\"\"Словарь кодировки лейблов\"\"\"\n",
    "    if x not in encode_dict.keys():\n",
    "        encode_dict[x]=len(encode_dict)\n",
    "    return encode_dict[x]\n",
    "\n",
    "df['ENCODE_CAT'] = df['label'].apply(lambda x: encode_cat(x))\n",
    "df = df[['text', 'ENCODE_CAT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdec1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Основные параметры для обучения Bert\n",
    "MAX_LEN = 256 \n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased-conversational')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8735ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    '''Даталоадер'''\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data.text[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.ENCODE_CAT[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d7ad540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (3848, 2)\n",
      "TRAIN Dataset: (3348, 2)\n",
      "TEST Dataset: (150, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.87\n",
    "train_dataset=df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "496a09dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca2f5737",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = BertModel.from_pretrained('DeepPavlov/rubert-base-cased-conversational')\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 17)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1229eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased-conversational were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = 'cpu'\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4a3eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aea67e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "848588f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a84a7fc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 2.783912420272827\n",
      "Training Accuracy per 5000 steps: 0.0\n",
      "The Total Accuracy for Epoch 0: 79.62962962962963\n",
      "Training Loss Epoch: 0.9143766787498834\n",
      "Training Accuracy Epoch: 79.62962962962963\n",
      "Training Loss per 5000 steps: 0.16351543366909027\n",
      "Training Accuracy per 5000 steps: 100.0\n",
      "The Total Accuracy for Epoch 1: 94.35483870967742\n",
      "Training Loss Epoch: 0.300846173981458\n",
      "Training Accuracy Epoch: 94.35483870967742\n",
      "Training Loss per 5000 steps: 0.05974167585372925\n",
      "Training Accuracy per 5000 steps: 100.0\n",
      "The Total Accuracy for Epoch 2: 95.48984468339307\n",
      "Training Loss Epoch: 0.2117280488331449\n",
      "Training Accuracy Epoch: 95.48984468339307\n",
      "Training Loss per 5000 steps: 0.14980089664459229\n",
      "Training Accuracy per 5000 steps: 100.0\n",
      "The Total Accuracy for Epoch 3: 96.50537634408602\n",
      "Training Loss Epoch: 0.15211264612619946\n",
      "Training Accuracy Epoch: 96.50537634408602\n",
      "Training Loss per 5000 steps: 0.06378297507762909\n",
      "Training Accuracy per 5000 steps: 100.0\n",
      "The Total Accuracy for Epoch 4: 97.16248506571087\n",
      "Training Loss Epoch: 0.11667769067856286\n",
      "Training Accuracy Epoch: 97.16248506571087\n",
      "Training Loss per 5000 steps: 0.026467783376574516\n",
      "Training Accuracy per 5000 steps: 100.0\n",
      "The Total Accuracy for Epoch 5: 98.11827956989248\n",
      "Training Loss Epoch: 0.08481207458404624\n",
      "Training Accuracy Epoch: 98.11827956989248\n",
      "Training Loss per 5000 steps: 0.022713065147399902\n",
      "Training Accuracy per 5000 steps: 100.0\n",
      "The Total Accuracy for Epoch 6: 98.47670250896057\n",
      "Training Loss Epoch: 0.06285986164640324\n",
      "Training Accuracy Epoch: 98.47670250896057\n",
      "Training Loss per 5000 steps: 0.011242968030273914\n",
      "Training Accuracy per 5000 steps: 100.0\n",
      "The Total Accuracy for Epoch 7: 98.53643966547192\n",
      "Training Loss Epoch: 0.0460550021018993\n",
      "Training Accuracy Epoch: 98.53643966547192\n",
      "Training Loss per 5000 steps: 0.024117205291986465\n",
      "Training Accuracy per 5000 steps: 100.0\n",
      "The Total Accuracy for Epoch 8: 99.19354838709677\n",
      "Training Loss Epoch: 0.02957446036970372\n",
      "Training Accuracy Epoch: 99.19354838709677\n",
      "Training Loss per 5000 steps: 0.002934409072622657\n",
      "Training Accuracy per 5000 steps: 100.0\n",
      "The Total Accuracy for Epoch 9: 99.31302270011948\n",
      "Training Loss Epoch: 0.023012819165655084\n",
      "Training Accuracy Epoch: 99.31302270011948\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10dc996e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def valid(model, testing_loader):\\n    model.eval()\\n    n_correct = 0; n_wrong = 0; total = 0\\n    tr_loss = 0\\n    nb_tr_steps=0\\n    nb_tr_examples=0\\n    with torch.no_grad():\\n        for _, data in enumerate(testing_loader, 0):\\n            ids = data[\\'ids\\'].to(device, dtype = torch.long)\\n            mask = data[\\'mask\\'].to(device, dtype = torch.long)\\n            targets = data[\\'targets\\'].to(device, dtype = torch.long)\\n            outputs = model(ids, mask).squeeze()\\n            loss = loss_function(outputs, targets)\\n            tr_loss += loss.item()\\n            big_val, big_idx = torch.max(outputs.data, dim=1)\\n            n_correct += calcuate_accu(big_idx, targets)\\n\\n            nb_tr_steps += 1\\n            nb_tr_examples+=targets.size(0)\\n            \\n            if _%5000==0:\\n                loss_step = tr_loss/nb_tr_steps\\n                accu_step = (n_correct*100)/nb_tr_examples\\n                print(f\"Validation Loss per 100 steps: {loss_step}\")\\n                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\\n    epoch_loss = tr_loss/nb_tr_steps\\n    epoch_accu = (n_correct*100)/nb_tr_examples\\n    print(f\"Validation Loss Epoch: {epoch_loss}\")\\n    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\\n    \\n    return epoch_accu'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Валидация для test\n",
    "\n",
    "'''def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps=0\n",
    "    nb_tr_examples=0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8f890a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные сохранены\n"
     ]
    }
   ],
   "source": [
    "output_model_file = 'pytorch_bert.bin'\n",
    "output_vocab_file = 'vocab_bert.bin'\n",
    "\n",
    "model_to_save = model\n",
    "torch.save(model_to_save, output_model_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)\n",
    "\n",
    "print('Данные сохранены')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "287ef2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_set = Triage(test_data, tokenizer, MAX_LEN)\n",
    "test_params = {'batch_size': 8,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b9c0f",
   "metadata": {},
   "source": [
    "## Валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da478bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask).squeeze()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            all_preds.append(big_idx)           \n",
    "    \n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db9f50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = valid(model, testing_loader)\n",
    "flat_list = [x.tolist() for xs in all_preds for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c443b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new=[]\n",
    "for search in flat_list:\n",
    "    for lbl, enc in encode_dict.items():\n",
    "        if enc == search:\n",
    "            new.append(lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "497b8fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(\"test.csv\")\n",
    "test_dataset = test_dataset[['id']]\n",
    "test_dataset['Категория']=new\n",
    "test_dataset.to_csv('sample_solution.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
